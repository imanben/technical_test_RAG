{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a74bdcf",
      "metadata": {
        "id": "9a74bdcf"
      },
      "source": [
        "# Technical Test : Minimal RAG Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "by Iman Bensalami"
      ],
      "metadata": {
        "id": "23V7zAf-Vdsd"
      },
      "id": "23V7zAf-Vdsd"
    },
    {
      "cell_type": "markdown",
      "id": "4a00957d",
      "metadata": {
        "id": "4a00957d"
      },
      "source": [
        "This notebook presents the implementation of a minimal Retrieval-Augmented Generation (RAG) pipeline, as part of the technical assessment.  \n",
        "This implementation is based solely on the publication available at https://arxiv.org/abs/2411.15594, which serves as the unique knowledge source for the retrieval component."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The pipeline follows the standard RAG structure:\n",
        "1. Preparation phase\n",
        "2. PDF preprocessing\n",
        "3. Chunking Phase\n",
        "4. Embedding and Indexing\n",
        "\n",
        "4. Prompt construction\n",
        "5. Generation with an Open-Source Language Model\n",
        "\n",
        "\n",
        "The code is fully structured, modular, and commented in English. It is designed to be simple, efficient, and easily testable. All dependencies are open-source.\n"
      ],
      "metadata": {
        "id": "sM88zT8cR_cD"
      },
      "id": "sM88zT8cR_cD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running this notebook, please make sure the following Python packages are installed.  You can install all dependencies at once using the requirements.txt file:"
      ],
      "metadata": {
        "id": "a6r44VP9yCo0"
      },
      "id": "a6r44VP9yCo0"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install PyMuPDF\n",
        "#!pip install faiss-cpu\n",
        "#!pip install transformers accelerate sentencepiece"
      ],
      "metadata": {
        "id": "dg6H0sj67HIz",
        "collapsed": true
      },
      "id": "dg6H0sj67HIz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5e69c987",
      "metadata": {
        "id": "5e69c987"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import fitz\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import faiss\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b49fa8e",
      "metadata": {
        "id": "0b49fa8e"
      },
      "source": [
        "## Preparation Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Selection and Analysis\n",
        "\n",
        "For this RAG pipeline, we exclusively use the scientific publication [“A Survey on LLM-as-a-Judge” (arXiv:2411.15594)](https://arxiv.org/abs/2411.15594) as the source document. This choice ensures that all answers produced by the system are grounded in a single, reliable, and domain-specific source.\n",
        "\n",
        "### Document Analysis\n",
        "\n",
        "The paper is a comprehensive survey that reviews the emerging concept of using large language models (LLMs) as evaluators. Its structure is detailed and systematic, following a clear academic organization:\n",
        "\n",
        "- An **abstract** that summarizes the goals, challenges, and contributions of the paper\n",
        "- A **well-structured hierarchy of sections and subsections** (e.g., `1`, `2.1`, `3.2.1`) covering definitions, methodologies, improvements, evaluation strategies, and future directions\n",
        "- Rich technical content that includes equations, comparative analyses, and implementation suggestions\n",
        "\n",
        "### Strengths\n",
        "\n",
        "- **Logical structure**: Sections are clearly numbered, making them ideal anchors for content chunking and metadata assignment.\n",
        "- **Question-answer alignment**: The main research questions of the survey directly support RAG-based QA.\n",
        "- **Domain depth**: The paper covers practical use cases, benchmark construction, prompt design strategies, and evaluator models, providing diverse yet connected knowledge points for retrieval.\n",
        "\n",
        "### Challenges\n",
        "\n",
        "- **Length and redundancy**: The document is long (~50 pages) and includes repeated phrasing or overlapping content, which can lead to redundancy in chunk embeddings.\n",
        "- **Technical noise**: Some segments contain citations, footnotes, or formatting artifacts which must be filtered out before chunking and indexing.\n",
        "- **Non-uniform section lengths**: Some sections (e.g., `5.2`) are very short and contain only titles or bullet points, which are not meaningful standalone for retrieval or answering.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QlV6788txK08"
      },
      "id": "QlV6788txK08"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Preprocessing"
      ],
      "metadata": {
        "id": "KPIr072b0Z4l"
      },
      "id": "KPIr072b0Z4l"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "853baca7",
      "metadata": {
        "id": "853baca7"
      },
      "outputs": [],
      "source": [
        "#pdf to text conversion\n",
        "# This function uses PyMuPDF (fitz) to extract raw text (even the text within the images) from each page of the given PDF.\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "#Text extraction\n",
        "pdf_path=\"publication.pdf\"\n",
        "text = extract_text_from_pdf(pdf_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What was removed and why:\n",
        "Before chunking and indexing the document, we apply a light but essential preprocessing step to ensure that the content used for retrieval is meaningful and free from irrelevant noise.\n",
        "\n",
        "- **Table of Contents**: Automatically generated tables of contents in academic papers often repeat section titles and page numbers, which add no semantic value for question answering and may skew chunk embeddings.\n",
        "  \n",
        "- **Headers and Footers**: We removed recurring elements such as \"A Survey on LLM-as-a-Judge\" and \"Vol. 1, No. 1, Article . Publication date: March 2025.\" These elements are repeated across pages and contain no domain-specific knowledge. Keeping them would introduce unnecessary noise into both the chunks and the retrieval index.\n",
        "\n",
        "- **In-text Citations**: We also removed inline references such as `[1]`, `[2]`, etc. These citations interrupt sentence flow, are not informative by themselves, and can negatively affect embedding clarity or mislead the retriever.\n",
        "- **Page-End Markers (.\\n\\n)**: We removed artificial page-end markers like .\\n\\n because they often break coherent paragraphs into separate fragments, disrupting the semantic flow of the text. Cleaning them ensures smoother chunking and improves embedding and retrieval quality in the RAG pipeline.\n",
        "\n",
        "- **Bibliography**: The reference section was excluded from the pipeline. It contains citation metadata that is not useful for answering domain-specific questions and may introduce noise or misleading retrieval results if indexed.\n",
        "\n",
        "This selective cleaning ensures that our RAG pipeline focuses purely on the substantive academic content of the publication, improving both embedding quality and final response relevance.\n"
      ],
      "metadata": {
        "id": "e-GUrqWh1e4t"
      },
      "id": "e-GUrqWh1e4t"
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove the table of contents section from the raw text\n",
        "def remove_table_of_contents(text):\n",
        "    start_match = re.search(r\"\\bContents\\b\", text) #match the exact word \"Contents\" using word boundaries (\\b)\n",
        "    end_match = re.search(r\"\\n\\s*2\\s+BACKGROUND AND METHOD\", text) # match the beginning of the nexte section, used as the end boundary of the table of contents\n",
        "\n",
        "    if start_match and end_match and end_match.start() > start_match.end():\n",
        "        return text[:start_match.start()] + text[end_match.start():]\n",
        "\n",
        "    return text #else return the original text\n",
        "\n",
        "\n",
        "text =remove_table_of_contents(text)"
      ],
      "metadata": {
        "id": "5LRDKMX_2jBw"
      },
      "id": "5LRDKMX_2jBw",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing recurring headers and footers from the text\n",
        "patterns = [\", Vol. 1, No. 1, Article . Publication date: March 2025\",\"J. Gu, X. Jiang, Z. Shi, J. Guo, et al.\", \"A Survey on LLM-as-a-Judge\"]\n",
        "\n",
        "def remove_headers_footers(text, patterns):\n",
        "  for pattern in patterns:\n",
        "    text = re.sub(pattern, '', text) #replace the pattenr with ''\n",
        "  return text\n",
        "\n",
        "\n",
        "text = remove_headers_footers(text, patterns)"
      ],
      "metadata": {
        "id": "_R3-a9tnDEsb"
      },
      "id": "_R3-a9tnDEsb",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove inline citations\n",
        "def remove_inline_citations(text):\n",
        "    return re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text)\n",
        "    ''' the pattern matches:\n",
        "            \\[ and \\] : an opening AND closing square bracket\n",
        "            \\d+      : one or more digits (e.g., \"1\")\n",
        "            (,\\s*\\d+)* : zero or more occurrences of a comma followed by optional whitespace and more digits\n",
        "    '''\n",
        "\n",
        "\n",
        "text=remove_inline_citations(text)"
      ],
      "metadata": {
        "id": "jbclz9FJ9Ezq"
      },
      "id": "jbclz9FJ9Ezq",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove artificial page-end markers .\\n\\n\") from the text\n",
        "def remove_page_end_markers(text):\n",
        "    return re.sub(r'\\.\\s*\\n\\s*\\n', '. ', text)\n",
        "\n",
        "\n",
        "text=remove_page_end_markers(text)"
      ],
      "metadata": {
        "id": "Yy0u4quo9GM-"
      },
      "id": "Yy0u4quo9GM-",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove the \"References\" section from the document\n",
        "def remove_references_section(text):\n",
        "    # Mot-clé de début de la section références (en majuscules ou minuscules)\n",
        "    pattern = r'\\n\\s*REFERENCES\\s*\\n'  # Define the pattern to locate the start of the \"REFERENCES\" section\n",
        "\n",
        "    match = re.search(pattern, text, flags=re.IGNORECASE)\n",
        "    if match:\n",
        "        return text[:match.start()].strip() #return everything before it\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "\n",
        "text=remove_references_section(text)"
      ],
      "metadata": {
        "id": "FhPVS5NOGF7X"
      },
      "id": "FhPVS5NOGF7X",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Removal of Image-Derived Text\n",
        "\n",
        "After the automated cleaning steps, I manually reviewed the text and removed content derived from images. These image-based segments often included figure captions and visual annotations that were not semantically relevant for retrieval or generation tasks within the RAG pipeline.\n",
        "\n",
        "This step could not be fully automated, as image-derived text is often irregular and inconsistently extracted by PDF parsers. Therefore, manual review was necessary to ensure high-quality input for chunking and embedding."
      ],
      "metadata": {
        "id": "qkGR5kxI8SdT"
      },
      "id": "qkGR5kxI8SdT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned text to a .txt file for manual editing\n",
        "with open(\"publication_preprocessed.txt\", \"w\") as f:\n",
        "    f.write(text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "7wy8so-4JFyY"
      },
      "id": "7wy8so-4JFyY",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#manual removal of image-derived segments such as figure captions and text from the images\n",
        "with open(\"publication_clean.txt\", \"r\") as f:\n",
        "    clean_text = f.read()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "oYwyVI2jnYLE"
      },
      "id": "oYwyVI2jnYLE",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking Phase"
      ],
      "metadata": {
        "id": "qA0VAfTjx6PV"
      },
      "id": "qA0VAfTjx6PV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logical Paragraph-Based Chunking with Document Structure\n",
        "\n",
        "To build an efficient and interpretable RAG pipeline, I implemented a custom chunking strategy based on the **logical structure of the document**. Instead of splitting the text arbitrarily (e.g., by fixed token length), I chose to segment it according to **explicit section and subsection headers** (e.g., `1`, `2.1`, `2.2.3`).\n",
        "\n",
        "This approach offers several key advantages:\n",
        "\n",
        "-**Semantic coherence**: Each chunk represents a complete and self-contained idea, improving retrieval relevance and generation quality.  \n",
        "-**Traceability**: Chunks preserve their original section titles as metadata, making it easier to map answers back to their source.\n",
        "\n",
        "In addition, I added the section number and its first sentence as the chunk title. This enhances navigation and makes it easier to understand the content of each chunk at a glance.\n"
      ],
      "metadata": {
        "id": "px0dNC8uB8DF"
      },
      "id": "px0dNC8uB8DF"
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_by_section(text):\n",
        "    lines = text.split('\\n')\n",
        "    heading_pattern = r'^\\d+(\\.\\d+)*\\s*$' # Pattern to match section headers like \"1\", \"2.3\", \"4.2.1\"\n",
        "\n",
        "    chunks = []\n",
        "    current_section_lines = []\n",
        "    current_section_number = None\n",
        "    abstract_lines = []  # Lines collected before any numbered section : the abstract\n",
        "\n",
        "    started_sections = False\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        if re.match(heading_pattern, line.strip()):\n",
        "            if not started_sections:\n",
        "                # First time we find a section heading ---> capture abstract\n",
        "                abstract_text = \"\\n\".join(abstract_lines).strip()\n",
        "                if abstract_text:\n",
        "                    chunks.append({\n",
        "                        \"title\": \"Abstract\",\n",
        "                        \"content\": abstract_text\n",
        "                    })\n",
        "                started_sections = True\n",
        "\n",
        "            # If we already have a section in progress, save it as a chunk\n",
        "            if current_section_number is not None and current_section_lines:\n",
        "                full_text = \"\\n\".join(current_section_lines).strip()\n",
        "                first_line = full_text.split('\\n', 1)[0]\n",
        "                first_sentence = first_line.split('.', 1)[0].strip()\n",
        "                title = f\"{current_section_number} - {first_sentence}\" if first_sentence else current_section_number\n",
        "                chunks.append({\n",
        "                    \"title\": title,\n",
        "                    \"content\": full_text\n",
        "                })\n",
        "\n",
        "            # Start a new section\n",
        "            current_section_number = line.strip()\n",
        "            current_section_lines = []\n",
        "\n",
        "        else:\n",
        "            # Collect lines into either the abstract or the current section\n",
        "            if started_sections:\n",
        "                current_section_lines.append(line)\n",
        "            else:\n",
        "                abstract_lines.append(line)\n",
        "\n",
        "    # Don't forget the last section at the end of the document\n",
        "    if current_section_number is not None and current_section_lines:\n",
        "        full_text = \"\\n\".join(current_section_lines).strip()\n",
        "        first_line = full_text.split('\\n', 1)[0]\n",
        "        first_sentence = first_line.split('.', 1)[0].strip()\n",
        "        title = f\"{current_section_number} - {first_sentence}\" if first_sentence else current_section_number\n",
        "        chunks.append({\n",
        "            \"title\": title,\n",
        "            \"content\": full_text\n",
        "        })\n",
        "\n",
        "    return chunks #list of dictionaries\n"
      ],
      "metadata": {
        "id": "bmGW0iyNi5-D"
      },
      "id": "bmGW0iyNi5-D",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunk_by_section(clean_text)"
      ],
      "metadata": {
        "id": "7zTEmP-frbcH"
      },
      "id": "7zTEmP-frbcH",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chunks_preview_notebook = pd.DataFrame(chunks[:5])\n",
        "df_chunks_preview_notebook.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IHZPF4LeEbBn",
        "outputId": "5c7272e1-1d9e-48ed-cb54-afbb149eec60"
      },
      "id": "IHZPF4LeEbBn",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       title  \\\n",
              "0                   Abstract   \n",
              "1           1 - INTRODUCTION   \n",
              "2  2 - BACKGROUND AND METHOD   \n",
              "3  2.1 - In-Context Learning   \n",
              "4  2.1.1 - Generating scores   \n",
              "\n",
              "                                             content  \n",
              "0  JIAWEI GU1,*, XUHUI JIANG1,*, ZHICHAO SHI1,2,*...  \n",
              "1  INTRODUCTION\\nJudgment is the faculty of think...  \n",
              "2  BACKGROUND AND METHOD\\nThe capacity of LLMs to...  \n",
              "3  In-Context Learning\\nTo apply LLM-as-a-Judge, ...  \n",
              "4  Generating scores. It is quite intuitive to re...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2f426af-be59-4a6b-8931-88a034ad0233\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abstract</td>\n",
              "      <td>JIAWEI GU1,*, XUHUI JIANG1,*, ZHICHAO SHI1,2,*...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1 - INTRODUCTION</td>\n",
              "      <td>INTRODUCTION\\nJudgment is the faculty of think...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2 - BACKGROUND AND METHOD</td>\n",
              "      <td>BACKGROUND AND METHOD\\nThe capacity of LLMs to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.1 - In-Context Learning</td>\n",
              "      <td>In-Context Learning\\nTo apply LLM-as-a-Judge, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.1.1 - Generating scores</td>\n",
              "      <td>Generating scores. It is quite intuitive to re...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2f426af-be59-4a6b-8931-88a034ad0233')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f2f426af-be59-4a6b-8931-88a034ad0233 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f2f426af-be59-4a6b-8931-88a034ad0233');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7152476a-f895-47d3-82b8-b9a3075dc82a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7152476a-f895-47d3-82b8-b9a3075dc82a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7152476a-f895-47d3-82b8-b9a3075dc82a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_chunks_preview_notebook",
              "summary": "{\n  \"name\": \"df_chunks_preview_notebook\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1 - INTRODUCTION\",\n          \"2.1.1 - Generating scores\",\n          \"2 - BACKGROUND AND METHOD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"INTRODUCTION\\nJudgment is the faculty of thinking the particular as contained under the universal. It\\ninvolves the capacity to subsume under rules, that is, to distinguish whether something\\nfalls under a given rule.\\n\\u2014\\u2014 Kant, Critique of Judgment , Introduction IV, 5:179; Critique of Pure Reason , A132/B171.\\nRecently, Large Language Models (LLMs) have achieved remarkable success across numerous\\ndomains , ranging from technical fields  to the humanities \\nand social sciences . Building on their success, the concept of using LLMs as\\nevaluators\\u2014commonly referred to as \\\"LLM-as-a-Judge\\\" \\u2014has gained significant attention,\\nwhere LLMs are tasked with determining whether something falls within the scope of a given\\nrule . This growing interest stems from LLMs\\u2019 ability to mimic human-like reasoning\\nand thinking processes, enabling them to take on roles traditionally reserved for human experts\\nwhile offering a cost-effective solution that can be effortlessly scaled to meet increasing evaluation demands. For instance, employing LLM-as-a-Judge in the academic peer-review1 process can help\\nhandle the rapid increase in submissions while maintaining expert-level judgment.\\nBefore the era of LLMs, finding a balance between comprehensive and scalable evaluation\\nposed a persistent challenge. On the one hand, widely used subjective methods like expert-driven\\nassessments  integrate holistic reasoning and fine-grained contextual understanding,\\nmaking them the gold standard in comprehensiveness. However, these approaches are costly,\\ndifficult to scale, and susceptible to inconsistency. On the other hand, objective assessment methods,\\nsuch as automatic metrics offer strong scalability and consistency. For example, tools such as\\nBLEU  or ROUGE  can rapidly evaluate machine-generated translations or summaries\\nagainst reference texts without human intervention. However, these metrics, which heavily rely on\\nsurface-level lexical overlaps, often fail to capture deeper nuances, resulting in poor performance\\nin tasks like story generation or instructional texts . As a solution to this persistent dilemma,\\n\\u201cLLM-as-a-Judge\\u201d has emerged as a promising idea to combine the strengths of the above two\\nevaluation methods. Recent studies have shown that this idea can merges the scalability of automatic\\nmethods with the detailed, context-sensitive reasoning found in expert judgments . Moreover, LLMs may become sufficiently flexible to handle multimodal inputs  under\\nappropriate prompt learning or fine-tuning . These advantages suggest that the LLM-as-a-Judge\\napproach could serve as a novel and broadly applicable paradigm for addressing complex and\\nopen-ended evaluation problems.\\nLLM-as-a-Judge holds significant potential as a scalable and adaptable evaluation framework\\ncompared to aforementioned two traditional methods . However, the widespread application\\nof this idea is hindered by two key challenges. The first challenge lies in the absence of a systematic\\nreview, which highlights the lack of formal definitions, fragmented understanding, and inconsistent\\nusage practices in the relevant studies. As a result, researchers and practitioners struggle to\\nfully understand and apply effectively. The second challenge involves addressing concerns about\\nreliability , as merely employing LLM-as-a-Judge does not ensure accurate evaluations aligned\\nwith established standards. These challenges emphasize the need for a deeper assessment of the\\noutputs generated by LLM-as-a-Judge, as well as a crucial investigation into the question: How to\\nbuild reliable LLM-as-a-Judge systems?\\nTo address these challenges, this paper provides a systematic review of research on LLM-as-a-\\nJudge. It offers a comprehensive overview of the field and explores strategies for building reliable\\nLLM-as-a-Judge systems. We begin by defining LLM-as-a-Judge through both formal and informal\\ndefinitions, answering the foundational question: \\\"What is LLM-as-a-Judge?\\\" Next, we categorize\\nexisting methods and approaches, exploring \\\"How to use LLM-as-a-Judge?\\\". Following this, toFigure 2.\\ntackle the critical question: \\\"How to build reliable LLM-as-a-Judge systems?\\\", we explore two core\\naspects: (1) strategies to enhance the reliability of LLM-as-a-Judge systems and (2) methodologies\\nfor evaluating the reliability of these systems. For the first aspect, we review key strategies to\\noptimize the performance of LLM-as-a-Judge. For the second aspect, we examine the metrics,\\ndatasets, and methodologies used to evaluate LLM-as-a-Judge systems, highlighting potential\\nsources of bias and methods for their mitigation. Building on this, we introduce a novel benchmark\\nspecifically designed for evaluating LLM-as-a-Judge systems. Additionally, we explore practical\\napplication scenarios and identify challenges unique to each context. Finally, we discuss future\\nresearch directions, emphasizing key areas for improving reliability, scalability, and applicability.\\nThe rest of this survey is organized as Figure 1. Section 2 provides an overview of the LLM-as-\\na-Judge field, including its definitions and categorization of existing methods. For a quick guide\\non the implementation of an LLM as a judge for specific scenarios, you can find answers in Quick\\n1https://blog.iclr.cc/2024/10/09/iclr2025-assisting-reviewers/\\n. Practice (2.5). Strategies for enhancing and evaluating the reliability of LLM-as-a-Judge systems\\nare discussed in Sections 3, 4, and 5, respectively. Notably, in Section 6, we discuss the synergy\\nbetween LLM-as-a-Judge and o1-like reasoning enhancement, where dynamic feedback is used to\\noptimize reasoning paths and significantly improve the model\\u2019s ability to solve complex problems.\\nSection 7 explores practical applications, while Sections 8 and 9 address challenges and outline\\nfuture research directions. Finally, Section 10 presents our conclusions.\",\n          \"Generating scores. It is quite intuitive to represent an evaluation using a corresponding\\nscore. What requires more careful consideration, however, is the nature and range of the score used\\nfor evaluation. The score can be discrete, with common ranges like 1-3, 1-5 , or 1-10 .\\nAlternatively, it can be continuous, ranging from 0 to 1 or 0 to 100 . The simplest way to\\nscore is through the context, setting the range of scores and the main criteria for scoring. For\\nexample, \\\"Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each\\nassistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall\\nperformance\\\" . A slightly more complex way is to provide more detailed scoring criteria. More\\ncomplex scoring situations can be as Language-Model-as-an-Examiner , which use Likert scale\\nscoring functions as an absolute evaluative measure, showed in Figure 4. The evaluator assigns\\nscores to a given response along predefined dimensions including accuracy, coherence, factuality\\nand comprehensiveness. Each of these dimensions is scored on a scale of 1 to 3, ranging from worst to best. The evaluator is also asked to provide an overall score ranging from 1 to 5, based on the\\nscores assigned to the previous 4 dimensions. This score serves as an indicator of the overall quality\\nof the answer.\",\n          \"BACKGROUND AND METHOD\\nThe capacity of LLMs to emulate human reasoning and evaluate specific inputs against a set of\\npredefined rules has paved the way for \\\"LLM-as-a-Judge.\\\" Existing studies indicate that LLM\\u2019s\\nscalability, adaptability, and cost-effectiveness make them well-suited for managing a growing\\nnumber of evaluative tasks that were traditionally done by humans. These abilities are key in\\nutilizing LLMs flexibly across various evaluation scenarios and objectives. As a result, adoption\\nof LLM in evaluation has progressed rapidly in practice. Initially, the primary focus of LLMs\\nwas on language generation and comprehension. With advancements in training paradigms like\\nReinforcement Learning from Human Feedback (RLHF) , LLMs became increasingly aligned\\nwith human values and reasoning processes. This alignment has allowed LLMs to transition from\\ngenerative tasks to evaluative roles. At its core, LLM-as-a-Judge denotes the use of LLMs to evaluate\\nobjects, actions, or decisions based on predefined rules, criteria, or preferences. It encompasses a\\nbroad spectrum of roles, including: Graders , Evaluators/Assessors , Critics , Verifiers , Examiners , Reward/Ranking Models ,\\netc.\\nCurrently, the definition of how to effectively use LLM-as-a-Judge for evaluation tasks is largely\\ninformal or vague, lacking clear and formal expression. Therefore, we will start with a formal\\ndefinition of LLM-as-Evaluator as follows:\\nE \\u2190PLLM (\\ud835\\udc65\\u2295C)\\n\\u2022 E: The final evaluation obtained from the whole LLM-as-a-Judge process in the expected\\nmanner. It could be a score, a choice, a label or a sentence, etc.\\n\\u2022 PLLM: The probability function defined by the corresponding LLM, and the generation is\\nan auto-regressive process.\\n\\u2022 \\ud835\\udc65: The input data in any available types (text, image, video), which waiting to be evaluated.\\n\\u2022 C: The context for the input \\ud835\\udc65, which is often prompt template or combined with history\\ninformation in dialogue.\\n\\u2022 \\u2295: The combination operator combines the input \\ud835\\udc65with the context C, and this operation\\ncan vary depending on the context, such as being placed at the beginning, middle, or end.\\nThe formulation of LLM-as-a-Judge reflects that LLM is a type of auto-regressive generative model,\\nwhich generates subsequent content based on the context and then obtains target evaluation from it.\\nIt illustrates how we utilize LLM for evaluation tasks, encompassing input design, model selection,\\nand training, as well as output post-processing. The basic approaches of implementing LLM-as-\\na-Judge can be classified according to the formulation: In-Context Learning, Model Selection,\\nPost-processing Method, and Evaluation Pipeline, which concluded in Figure 2. By following this\\npipeline, one can build a basic LLM-as-a-Judge for evaluation. A quick practice guide is available in\\nsection 2.5.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This loop calculates the number of words in each chunk's content and adds it as a new field called \"word_count\".\n",
        "# This is useful for analyzing chunk size distribution, filtering very short or very long chunks,\n",
        "# and setting thresholds for embedding or model input length.\n",
        "for chunk in chunks:\n",
        "    chunk[\"word_count\"] = len(chunk[\"content\"].split())"
      ],
      "metadata": {
        "id": "THlHbtLsv_85"
      },
      "id": "THlHbtLsv_85",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chunks_preview_notebook = pd.DataFrame(chunks[:5])\n",
        "df_chunks_preview_notebook.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fbRceEsOFMfd",
        "outputId": "6e03c74b-81ab-43e5-ce3a-8b4c6111fada"
      },
      "id": "fbRceEsOFMfd",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       title  \\\n",
              "0                   Abstract   \n",
              "1           1 - INTRODUCTION   \n",
              "2  2 - BACKGROUND AND METHOD   \n",
              "3  2.1 - In-Context Learning   \n",
              "4  2.1.1 - Generating scores   \n",
              "\n",
              "                                             content  word_count  \n",
              "0  JIAWEI GU1,*, XUHUI JIANG1,*, ZHICHAO SHI1,2,*...         302  \n",
              "1  INTRODUCTION\\nJudgment is the faculty of think...         811  \n",
              "2  BACKGROUND AND METHOD\\nThe capacity of LLMs to...         421  \n",
              "3  In-Context Learning\\nTo apply LLM-as-a-Judge, ...         119  \n",
              "4  Generating scores. It is quite intuitive to re...         226  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b775f0e9-a2cf-4e80-a0e9-c5ab69136767\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abstract</td>\n",
              "      <td>JIAWEI GU1,*, XUHUI JIANG1,*, ZHICHAO SHI1,2,*...</td>\n",
              "      <td>302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1 - INTRODUCTION</td>\n",
              "      <td>INTRODUCTION\\nJudgment is the faculty of think...</td>\n",
              "      <td>811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2 - BACKGROUND AND METHOD</td>\n",
              "      <td>BACKGROUND AND METHOD\\nThe capacity of LLMs to...</td>\n",
              "      <td>421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.1 - In-Context Learning</td>\n",
              "      <td>In-Context Learning\\nTo apply LLM-as-a-Judge, ...</td>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.1.1 - Generating scores</td>\n",
              "      <td>Generating scores. It is quite intuitive to re...</td>\n",
              "      <td>226</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b775f0e9-a2cf-4e80-a0e9-c5ab69136767')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b775f0e9-a2cf-4e80-a0e9-c5ab69136767 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b775f0e9-a2cf-4e80-a0e9-c5ab69136767');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-96172898-6f05-4673-afb3-58d1907dbd79\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96172898-6f05-4673-afb3-58d1907dbd79')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-96172898-6f05-4673-afb3-58d1907dbd79 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_chunks_preview_notebook",
              "summary": "{\n  \"name\": \"df_chunks_preview_notebook\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1 - INTRODUCTION\",\n          \"2.1.1 - Generating scores\",\n          \"2 - BACKGROUND AND METHOD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"INTRODUCTION\\nJudgment is the faculty of thinking the particular as contained under the universal. It\\ninvolves the capacity to subsume under rules, that is, to distinguish whether something\\nfalls under a given rule.\\n\\u2014\\u2014 Kant, Critique of Judgment , Introduction IV, 5:179; Critique of Pure Reason , A132/B171.\\nRecently, Large Language Models (LLMs) have achieved remarkable success across numerous\\ndomains , ranging from technical fields  to the humanities \\nand social sciences . Building on their success, the concept of using LLMs as\\nevaluators\\u2014commonly referred to as \\\"LLM-as-a-Judge\\\" \\u2014has gained significant attention,\\nwhere LLMs are tasked with determining whether something falls within the scope of a given\\nrule . This growing interest stems from LLMs\\u2019 ability to mimic human-like reasoning\\nand thinking processes, enabling them to take on roles traditionally reserved for human experts\\nwhile offering a cost-effective solution that can be effortlessly scaled to meet increasing evaluation demands. For instance, employing LLM-as-a-Judge in the academic peer-review1 process can help\\nhandle the rapid increase in submissions while maintaining expert-level judgment.\\nBefore the era of LLMs, finding a balance between comprehensive and scalable evaluation\\nposed a persistent challenge. On the one hand, widely used subjective methods like expert-driven\\nassessments  integrate holistic reasoning and fine-grained contextual understanding,\\nmaking them the gold standard in comprehensiveness. However, these approaches are costly,\\ndifficult to scale, and susceptible to inconsistency. On the other hand, objective assessment methods,\\nsuch as automatic metrics offer strong scalability and consistency. For example, tools such as\\nBLEU  or ROUGE  can rapidly evaluate machine-generated translations or summaries\\nagainst reference texts without human intervention. However, these metrics, which heavily rely on\\nsurface-level lexical overlaps, often fail to capture deeper nuances, resulting in poor performance\\nin tasks like story generation or instructional texts . As a solution to this persistent dilemma,\\n\\u201cLLM-as-a-Judge\\u201d has emerged as a promising idea to combine the strengths of the above two\\nevaluation methods. Recent studies have shown that this idea can merges the scalability of automatic\\nmethods with the detailed, context-sensitive reasoning found in expert judgments . Moreover, LLMs may become sufficiently flexible to handle multimodal inputs  under\\nappropriate prompt learning or fine-tuning . These advantages suggest that the LLM-as-a-Judge\\napproach could serve as a novel and broadly applicable paradigm for addressing complex and\\nopen-ended evaluation problems.\\nLLM-as-a-Judge holds significant potential as a scalable and adaptable evaluation framework\\ncompared to aforementioned two traditional methods . However, the widespread application\\nof this idea is hindered by two key challenges. The first challenge lies in the absence of a systematic\\nreview, which highlights the lack of formal definitions, fragmented understanding, and inconsistent\\nusage practices in the relevant studies. As a result, researchers and practitioners struggle to\\nfully understand and apply effectively. The second challenge involves addressing concerns about\\nreliability , as merely employing LLM-as-a-Judge does not ensure accurate evaluations aligned\\nwith established standards. These challenges emphasize the need for a deeper assessment of the\\noutputs generated by LLM-as-a-Judge, as well as a crucial investigation into the question: How to\\nbuild reliable LLM-as-a-Judge systems?\\nTo address these challenges, this paper provides a systematic review of research on LLM-as-a-\\nJudge. It offers a comprehensive overview of the field and explores strategies for building reliable\\nLLM-as-a-Judge systems. We begin by defining LLM-as-a-Judge through both formal and informal\\ndefinitions, answering the foundational question: \\\"What is LLM-as-a-Judge?\\\" Next, we categorize\\nexisting methods and approaches, exploring \\\"How to use LLM-as-a-Judge?\\\". Following this, toFigure 2.\\ntackle the critical question: \\\"How to build reliable LLM-as-a-Judge systems?\\\", we explore two core\\naspects: (1) strategies to enhance the reliability of LLM-as-a-Judge systems and (2) methodologies\\nfor evaluating the reliability of these systems. For the first aspect, we review key strategies to\\noptimize the performance of LLM-as-a-Judge. For the second aspect, we examine the metrics,\\ndatasets, and methodologies used to evaluate LLM-as-a-Judge systems, highlighting potential\\nsources of bias and methods for their mitigation. Building on this, we introduce a novel benchmark\\nspecifically designed for evaluating LLM-as-a-Judge systems. Additionally, we explore practical\\napplication scenarios and identify challenges unique to each context. Finally, we discuss future\\nresearch directions, emphasizing key areas for improving reliability, scalability, and applicability.\\nThe rest of this survey is organized as Figure 1. Section 2 provides an overview of the LLM-as-\\na-Judge field, including its definitions and categorization of existing methods. For a quick guide\\non the implementation of an LLM as a judge for specific scenarios, you can find answers in Quick\\n1https://blog.iclr.cc/2024/10/09/iclr2025-assisting-reviewers/\\n. Practice (2.5). Strategies for enhancing and evaluating the reliability of LLM-as-a-Judge systems\\nare discussed in Sections 3, 4, and 5, respectively. Notably, in Section 6, we discuss the synergy\\nbetween LLM-as-a-Judge and o1-like reasoning enhancement, where dynamic feedback is used to\\noptimize reasoning paths and significantly improve the model\\u2019s ability to solve complex problems.\\nSection 7 explores practical applications, while Sections 8 and 9 address challenges and outline\\nfuture research directions. Finally, Section 10 presents our conclusions.\",\n          \"Generating scores. It is quite intuitive to represent an evaluation using a corresponding\\nscore. What requires more careful consideration, however, is the nature and range of the score used\\nfor evaluation. The score can be discrete, with common ranges like 1-3, 1-5 , or 1-10 .\\nAlternatively, it can be continuous, ranging from 0 to 1 or 0 to 100 . The simplest way to\\nscore is through the context, setting the range of scores and the main criteria for scoring. For\\nexample, \\\"Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each\\nassistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall\\nperformance\\\" . A slightly more complex way is to provide more detailed scoring criteria. More\\ncomplex scoring situations can be as Language-Model-as-an-Examiner , which use Likert scale\\nscoring functions as an absolute evaluative measure, showed in Figure 4. The evaluator assigns\\nscores to a given response along predefined dimensions including accuracy, coherence, factuality\\nand comprehensiveness. Each of these dimensions is scored on a scale of 1 to 3, ranging from worst to best. The evaluator is also asked to provide an overall score ranging from 1 to 5, based on the\\nscores assigned to the previous 4 dimensions. This score serves as an indicator of the overall quality\\nof the answer.\",\n          \"BACKGROUND AND METHOD\\nThe capacity of LLMs to emulate human reasoning and evaluate specific inputs against a set of\\npredefined rules has paved the way for \\\"LLM-as-a-Judge.\\\" Existing studies indicate that LLM\\u2019s\\nscalability, adaptability, and cost-effectiveness make them well-suited for managing a growing\\nnumber of evaluative tasks that were traditionally done by humans. These abilities are key in\\nutilizing LLMs flexibly across various evaluation scenarios and objectives. As a result, adoption\\nof LLM in evaluation has progressed rapidly in practice. Initially, the primary focus of LLMs\\nwas on language generation and comprehension. With advancements in training paradigms like\\nReinforcement Learning from Human Feedback (RLHF) , LLMs became increasingly aligned\\nwith human values and reasoning processes. This alignment has allowed LLMs to transition from\\ngenerative tasks to evaluative roles. At its core, LLM-as-a-Judge denotes the use of LLMs to evaluate\\nobjects, actions, or decisions based on predefined rules, criteria, or preferences. It encompasses a\\nbroad spectrum of roles, including: Graders , Evaluators/Assessors , Critics , Verifiers , Examiners , Reward/Ranking Models ,\\netc.\\nCurrently, the definition of how to effectively use LLM-as-a-Judge for evaluation tasks is largely\\ninformal or vague, lacking clear and formal expression. Therefore, we will start with a formal\\ndefinition of LLM-as-Evaluator as follows:\\nE \\u2190PLLM (\\ud835\\udc65\\u2295C)\\n\\u2022 E: The final evaluation obtained from the whole LLM-as-a-Judge process in the expected\\nmanner. It could be a score, a choice, a label or a sentence, etc.\\n\\u2022 PLLM: The probability function defined by the corresponding LLM, and the generation is\\nan auto-regressive process.\\n\\u2022 \\ud835\\udc65: The input data in any available types (text, image, video), which waiting to be evaluated.\\n\\u2022 C: The context for the input \\ud835\\udc65, which is often prompt template or combined with history\\ninformation in dialogue.\\n\\u2022 \\u2295: The combination operator combines the input \\ud835\\udc65with the context C, and this operation\\ncan vary depending on the context, such as being placed at the beginning, middle, or end.\\nThe formulation of LLM-as-a-Judge reflects that LLM is a type of auto-regressive generative model,\\nwhich generates subsequent content based on the context and then obtains target evaluation from it.\\nIt illustrates how we utilize LLM for evaluation tasks, encompassing input design, model selection,\\nand training, as well as output post-processing. The basic approaches of implementing LLM-as-\\na-Judge can be classified according to the formulation: In-Context Learning, Model Selection,\\nPost-processing Method, and Evaluation Pipeline, which concluded in Figure 2. By following this\\npipeline, one can build a basic LLM-as-a-Judge for evaluation. A quick practice guide is available in\\nsection 2.5.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 267,\n        \"min\": 119,\n        \"max\": 811,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          811,\n          226,\n          421\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering Short Chunks\n",
        "\n",
        "After chunking the document by logical sections, we filtered out all chunks with fewer than 6 words. Very short chunks often result from section headers, broken formatting, or incomplete paragraphs. These chunks rarely contain useful semantic information and can degrade the quality of retrieval and embedding.\n",
        "\n",
        "By applying this threshold, we ensure that each chunk has enough context to be meaningful during similarity search and language model generation, while also keeping the vector index cleaner and more efficient.\n"
      ],
      "metadata": {
        "id": "dRiHvPNPFpmz"
      },
      "id": "dRiHvPNPFpmz"
    },
    {
      "cell_type": "code",
      "source": [
        "# We define a minimum threshold of 6 words to retain only informative content.\n",
        "\n",
        "MIN_WORDS = 6  # Minimum number of words required for a chunk to be kept\n",
        "\n",
        "# Create a new list containing only chunks that meet the minimum word count\n",
        "filtered_chunks = [\n",
        "    chunk for chunk in chunks\n",
        "    if len(chunk[\"content\"].split()) >= MIN_WORDS\n",
        "]\n",
        "\n",
        "# Update the chunk list\n",
        "chunks = filtered_chunks"
      ],
      "metadata": {
        "id": "9-NpWndXyVyC"
      },
      "id": "9-NpWndXyVyC",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Long Chunks with Overlap\n",
        "\n",
        "To ensure each chunk fits within the maximum input length of embedding models or language models, we split any chunk exceeding 300 words into smaller, overlapping sub-chunks. We applied an overlap of 50 words between consecutive sub-chunks to preserve context across boundaries and avoid losing important transitional information.\n",
        "\n",
        "This approach strikes a balance between chunk size, model compatibility, and semantic continuity — improving both retrieval accuracy and generative coherence in the RAG pipeline.\n"
      ],
      "metadata": {
        "id": "9VW-jRKeGvQw"
      },
      "id": "9VW-jRKeGvQw"
    },
    {
      "cell_type": "code",
      "source": [
        "#Split long chunks into smaller overlapping sub-chunks (word-based)\n",
        "def split_chunks_by_words_list(chunks, max_words=300, overlap=50):\n",
        "    def split_one_chunk(text, title):\n",
        "        words = text.split()\n",
        "        sub_chunks = []\n",
        "        start = 0\n",
        "        index = 1\n",
        "\n",
        "        while start < len(words):  # Sliding window approach: move in steps with overlap\n",
        "            end = start + max_words\n",
        "            chunk_words = words[start:end]\n",
        "            chunk_text = \" \".join(chunk_words)\n",
        "            sub_chunks.append({\n",
        "                \"title\": f\"{title} [{index}]\",   # Sub-chunk titles are numbered\n",
        "                \"content\": chunk_text,\n",
        "                \"word_count\": len(chunk_words)\n",
        "            })\n",
        "            index += 1\n",
        "            start += max_words - overlap\n",
        "        return sub_chunks\n",
        "\n",
        "    final_chunks = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        title = chunk[\"title\"]\n",
        "        content = chunk[\"content\"]\n",
        "        word_count = len(content.split())\n",
        "\n",
        "        if word_count > max_words: # If the chunk is too long, split it\n",
        "            final_chunks.extend(split_one_chunk(content, title))\n",
        "\n",
        "        else:\n",
        "            chunk[\"word_count\"] = word_count\n",
        "            final_chunks.append(chunk)\n",
        "\n",
        "    return final_chunks\n"
      ],
      "metadata": {
        "id": "iTuolN4-3q7X"
      },
      "id": "iTuolN4-3q7X",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_chunks = split_chunks_by_words_list(chunks)  # 'chunks' is a list of dictionaries (title + content+ word_count)\n",
        "\n",
        "# From this point on, 'chunks' refers to the length-constrained, overlap-aware version\n",
        "chunks = split_chunks"
      ],
      "metadata": {
        "id": "0K-9sMfj3-dW"
      },
      "id": "0K-9sMfj3-dW",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VISUALIZATION\n",
        "\n",
        "# Extract word counts\n",
        "word_counts = [chunk[\"word_count\"] for chunk in chunks]\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(word_counts, bins=20, edgecolor='black')\n",
        "plt.title(\"Distribution of Chunk Word Counts\")\n",
        "plt.xlabel(\"Number of Words per Chunk\")\n",
        "plt.ylabel(\"Number of Chunks\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "ER43RYOCHol1",
        "outputId": "2f04f0b6-9f36-4546-c930-502374a2a9ae"
      },
      "id": "ER43RYOCHol1",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding and Indexation"
      ],
      "metadata": {
        "id": "iKQFfRe3E-Kt"
      },
      "id": "iKQFfRe3E-Kt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enable efficient semantic search, each chunk was transformed into a vector using the `all-MiniLM-L6-v2` embedding model from the `sentence-transformers` library. We enriched the input by combining each chunk’s title with its content to enhance context understanding. These embeddings were stored in a FAISS index, which allows for fast and scalable similarity search. In parallel, we preserved metadata (title and content) to retrieve and display the full text of each chunk after retrieval. This step forms the backbone of the retrieval process in the RAG pipeline.\n"
      ],
      "metadata": {
        "id": "F-jZW31IJJsq"
      },
      "id": "F-jZW31IJJsq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the embedding model\n",
        "# We use a lightweight sentence transformer to convert text into semantic vector representations.\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Each text includes both the section title and its content to improve semantic context.\n",
        "texts = [f\"{chunk['title']}\\n{chunk['content']}\" for chunk in chunks]\n",
        "\n",
        "#generating embeddings for all chunks\n",
        "embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "5I6YUUpEDw0P"
      },
      "id": "5I6YUUpEDw0P",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FAISS index\n",
        "dimension = embeddings.shape[1] #This is required by FAISS to create an index with the appropriate vector size.\n",
        "\n",
        "index = faiss.IndexFlatL2(dimension) ## FAISS will store the embedding vectors and perform similarity search based on L2 distance.\n",
        "\n",
        "index.add(embeddings)  # Add all chunk vectors to the index"
      ],
      "metadata": {
        "id": "bP2qU1cXJkbj"
      },
      "id": "bP2qU1cXJkbj",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store metadata for retrieval\n",
        "metadata = [{\"title\": chunk[\"title\"], \"content\": chunk[\"content\"]} for chunk in chunks]"
      ],
      "metadata": {
        "id": "vVYSmj9ZKBVC"
      },
      "id": "vVYSmj9ZKBVC",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval Test: Query Embedding and Top-k Semantic Search"
      ],
      "metadata": {
        "id": "8ZOE2UWtKMyT"
      },
      "id": "8ZOE2UWtKMyT"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the limitations of using LLMs as judges?\"\n",
        "query_embedding = model.encode([query]) # This ensures the query and the chunks are in the same vector space.\n",
        "\n",
        "\n",
        "#Perform semantic search in the FAISS index\n",
        "# This retrieves the top 3 most similar chunks (based on L2 distance) to the query embedding.\n",
        "D, I = index.search(np.array(query_embedding), k=3)\n",
        " # I --> Index positions of top 3 matches in the original data\n",
        " # D --> Distances to those 3 matches (smaller = better)\n",
        "\n",
        "\n",
        "#Display the top-matching chunks\n",
        "for idx in I[0]:\n",
        "    print(metadata[idx][\"title\"])\n",
        "    print(metadata[idx][\"content\"][:300])\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6dL99v_7tKK",
        "outputId": "f3bd0933-dbc5-48fb-8373-93ec1c9e1c85"
      },
      "id": "N6dL99v_7tKK",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 - APPLICATIONS\n",
            "APPLICATIONS\n",
            "LLMs’ abilities as evaluators have gained widespread recognition in specialized fields, especially in\n",
            "complex, qualitative areas like legal texts, mathematical reasoning, and scientific research .\n",
            "This section reviews recent developments in LLM-as-a-judge applications across finance, la\n",
            "---\n",
            "8 - CHALLENGES\n",
            "CHALLENGES\n",
            "In this chapter, we explore the key challenges that arise when utilizing LLMs for evaluation\n",
            "tasks, particularly in the context of LLM-as-a-Judge. Despite their growing capabilities, LLMs still\n",
            "face significant issues related to reliability, robustness, and their backbone models’ limitati\n",
            "---\n",
            "Abstract [2]\n",
            "a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a- Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly \n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Construction"
      ],
      "metadata": {
        "id": "VUJGJZjk895S"
      },
      "id": "VUJGJZjk895S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Construction Strategy\n",
        "\n",
        "This function serves as the critical link between retrieval and generation in the RAG pipeline. It takes a user question, encodes it into a vector using the same embedding model used during indexing, and retrieves the top-k most semantically similar chunks from the FAISS index. These chunks are then formatted into a structured prompt."
      ],
      "metadata": {
        "id": "rmef-8YOMaJS"
      },
      "id": "rmef-8YOMaJS"
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_and_construct_prompt(question, model, index, metadata, top_k=5):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        prompt (str): A formatted prompt to be passed to the LLM.\n",
        "        selected_chunks (list): The retrieved chunks used in the prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    # Encode the question into a semantic vector\n",
        "    query_embedding = model.encode([question], convert_to_numpy=True)\n",
        "\n",
        "    # Perform top-k semantic search in the FAISS index\n",
        "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
        "    selected_chunks = [metadata[i] for i in indices[0]]\n",
        "\n",
        "    # Build a structured prompt including titles, content, and the question\n",
        "    prompt = \"### CONTEXT\\n\"\n",
        "    for chunk in selected_chunks:\n",
        "        prompt += f\"\\n## {chunk['title']}\\n{chunk['content']}\\n\"\n",
        "    prompt += f\"\\n### QUESTION\\n{question}\\n\"\n",
        "    prompt += \"### ANSWER :\\n\"\n",
        "\n",
        "    return prompt, selected_chunks\n"
      ],
      "metadata": {
        "id": "y35XjOUE89ot"
      },
      "id": "y35XjOUE89ot",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Construction test"
      ],
      "metadata": {
        "id": "Ydht4x86NZOv"
      },
      "id": "Ydht4x86NZOv"
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What are the limitations of using LLMs as judges?\"\n",
        "\n",
        "# Retrieve the top-5 most relevant chunks and construct a structured prompt\n",
        "prompt, sources = retrieve_and_construct_prompt(\n",
        "    question=user_question,\n",
        "    model=model,\n",
        "    index=index,\n",
        "    metadata=metadata,\n",
        "    top_k=5\n",
        ")\n",
        "\n",
        "print(prompt) # This prompt will be then passed to the language model for answer generation."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noPp0Mjk9JSg",
        "outputId": "44e3721b-acb9-43e3-c683-b439fc026d07"
      },
      "id": "noPp0Mjk9JSg",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### CONTEXT\n",
            "\n",
            "## 7 - APPLICATIONS\n",
            "APPLICATIONS\n",
            "LLMs’ abilities as evaluators have gained widespread recognition in specialized fields, especially in\n",
            "complex, qualitative areas like legal texts, mathematical reasoning, and scientific research .\n",
            "This section reviews recent developments in LLM-as-a-judge applications across finance, law,\n",
            "science, and other industries, investigating how domain knowledge and LLM evaluators can further\n",
            "expand their impact in critical areas.\n",
            "\n",
            "## 8 - CHALLENGES\n",
            "CHALLENGES\n",
            "In this chapter, we explore the key challenges that arise when utilizing LLMs for evaluation\n",
            "tasks, particularly in the context of LLM-as-a-Judge. Despite their growing capabilities, LLMs still\n",
            "face significant issues related to reliability, robustness, and their backbone models’ limitations.\n",
            "Understanding these challenges is crucial for advancing the use of LLMs in a fair, consistent, and\n",
            "reliable manner. We address these concerns under three main themes: reliability, robustness, and\n",
            "the need for more powerful backbone models.\n",
            "\n",
            "## Abstract [2]\n",
            "a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a- Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field. The associated resources can be accessed at https://awesome-llm-as-a-judge.github.io/ .\n",
            "\n",
            "## 1 - INTRODUCTION [3]\n",
            "systems? To address these challenges, this paper provides a systematic review of research on LLM-as-a- Judge. It offers a comprehensive overview of the field and explores strategies for building reliable LLM-as-a-Judge systems. We begin by defining LLM-as-a-Judge through both formal and informal definitions, answering the foundational question: \"What is LLM-as-a-Judge?\" Next, we categorize existing methods and approaches, exploring \"How to use LLM-as-a-Judge?\". Following this, toFigure 2. tackle the critical question: \"How to build reliable LLM-as-a-Judge systems?\", we explore two core aspects: (1) strategies to enhance the reliability of LLM-as-a-Judge systems and (2) methodologies for evaluating the reliability of these systems. For the first aspect, we review key strategies to optimize the performance of LLM-as-a-Judge. For the second aspect, we examine the metrics, datasets, and methodologies used to evaluate LLM-as-a-Judge systems, highlighting potential sources of bias and methods for their mitigation. Building on this, we introduce a novel benchmark specifically designed for evaluating LLM-as-a-Judge systems. Additionally, we explore practical application scenarios and identify challenges unique to each context. Finally, we discuss future research directions, emphasizing key areas for improving reliability, scalability, and applicability. The rest of this survey is organized as Figure 1. Section 2 provides an overview of the LLM-as- a-Judge field, including its definitions and categorization of existing methods. For a quick guide on the implementation of an LLM as a judge for specific scenarios, you can find answers in Quick 1https://blog.iclr.cc/2024/10/09/iclr2025-assisting-reviewers/ . Practice (2.5). Strategies for enhancing and evaluating the reliability of LLM-as-a-Judge systems are discussed in Sections 3, 4, and 5, respectively. Notably, in Section 6, we discuss the synergy between LLM-as-a-Judge and o1-like reasoning enhancement, where dynamic feedback is used to optimize reasoning paths and significantly improve the model’s ability to solve complex problems. Section 7 explores practical applications, while Sections 8 and 9 address challenges\n",
            "\n",
            "## 5.2.4 - Summary\n",
            "Summary. Due to the inherent capabilities and potential risks of LLMs, common improvement\n",
            "strategies for LLM-as-a-judge are not fully effective in improving the evaluation performance or\n",
            "mitigating biases. The limitations and challenges will be further discussed in Section 8.\n",
            "Based on the current experimental analysis, an empirical strategy for pairwise comparison\n",
            "evaluation tasks is to select more powerful LLMs and to adopt two evaluation strategies: one\n",
            "is swapping the positions of the evaluation contents, the other is taking the majority\n",
            "voting results from multiple rounds of evaluation, which can effectively mitigate biases. As\n",
            "for improving the alignment with humans, further exploration is still needed.\n",
            "\n",
            "### QUESTION\n",
            "What are the limitations of using LLMs as judges?\n",
            "### ANSWER :\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation with an Open-Source Language Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ehFUXu7a_i1T"
      },
      "id": "ehFUXu7a_i1T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete the RAG pipeline, we use an open-source generative model — TinyLlama 1.1B — loaded via Hugging Face's `transformers` library. The tokenizer converts text into model-ready tokens, while the model generates the answer based on the constructed prompt. We use the `pipeline(\"text-generation\")` utility for simplicity and readability.\n"
      ],
      "metadata": {
        "id": "0ufRHFfDOwxm"
      },
      "id": "0ufRHFfDOwxm"
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the open-source language model and tokenizer (TinyLlama)\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# device_map=\"auto\" will use GPU if available, otherwise CPU\n",
        "# torch_dtype=\"auto\" lets the model pick the most efficient data type\n",
        "model_generation = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "# Create a text-generation pipeline\n",
        "# This wraps the model and tokenizer into a single callable object soo we can pass in a prompt and receive generated text.\n",
        "generator = pipeline(\"text-generation\", model=model_generation, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrx2_SVq-VY4",
        "outputId": "77127c33-6efd-4b6f-dbf7-e7090d675f05"
      },
      "id": "Lrx2_SVq-VY4",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean Answer Generation\n",
        "\n",
        "To ensure the model’s output remains concise and relevant, we implemented a post-processing function that trims off unwanted continuations. Open-source models like TinyLlama often continue generating text beyond the answer (e.g., repeating the context or sections). To prevent this, we check for specific stop-tokens (e.g., `\"##\"`, `\"QUESTION\"`) and truncate the output accordingly. This approach improves clarity, avoids redundancy, and ensures the response stays focused on the original question.\n"
      ],
      "metadata": {
        "id": "mrazerJQPvMK"
      },
      "id": "mrazerJQPvMK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters breakdown:\n",
        "- Temperature 0.3 : Lower = safer, more conservative responses\n",
        "- max_new_tokens=350 : Limits the maximum number of tokens the model is allowed to generate\n",
        "- do_sample=False : Disables random sampling."
      ],
      "metadata": {
        "id": "rf_w5IKQSObE"
      },
      "id": "rf_w5IKQSObE"
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean answer generation with stop-token trimming\n",
        "def generate_answer_with_cleaning(generator, prompt, stop_tokens=[\"##\", \"###\", \"QUESTION\", \"CONTEXT\"]):\n",
        "\n",
        "    # Generate raw text from the language model\n",
        "    response = generator(prompt, max_new_tokens=350, do_sample=False, temperature=0.3)\n",
        "\n",
        "    # Extract only the new text generated after the original prompt\n",
        "    generated = response[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "    # Truncate the output at the first unwanted continuation marker (if any)\n",
        "    for stop_token in stop_tokens:\n",
        "        if stop_token in generated:\n",
        "            generated = generated.split(stop_token)[0].strip()\n",
        "\n",
        "    wrapped = textwrap.fill(generated, width=100, replace_whitespace=False)\n",
        "    return wrapped\n"
      ],
      "metadata": {
        "id": "EJ3U0hLzMyXf"
      },
      "id": "EJ3U0hLzMyXf",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the prompt is constructed and the model is loaded, we pass the prompt through the `generate_answer_with_cleaning` function. This ensures the response is not only generated accurately but also cleaned of any repeated sections or unwanted formatting. The final answer is then displayed, marking the completion of the Retrieval-Augmented Generation (RAG) pipeline.\n"
      ],
      "metadata": {
        "id": "nx7Yxg9dQLrG"
      },
      "id": "nx7Yxg9dQLrG"
    },
    {
      "cell_type": "code",
      "source": [
        "answer = generate_answer_with_cleaning(generator, prompt)\n",
        "\n",
        "print(\"GENERATED ANSWER:\\n\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NzsNWSyHGxd",
        "outputId": "51dc947e-eace-4f01-fb5d-5673c1e8d9a3"
      },
      "id": "3NzsNWSyHGxd",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED ANSWER:\n",
            "\n",
            "The limitations of using LLMs as judges are not fully effective in improving the evaluation\n",
            "performance or mitigating biases. The limitations and challenges will be further discussed in\n",
            "Section 8. Based on the current experimental analysis, an empirical strategy for pairwise comparison\n",
            "evaluation tasks is to select more powerful LLMs and to adopt two evaluation strategies: one is\n",
            "swapping the positions of the evaluation contents, the other is taking the majority voting results\n",
            "from multiple rounds of evaluation, which can effectively mitigate biases. As for improving the\n",
            "alignment with humans, further exploration is still needed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests"
      ],
      "metadata": {
        "id": "MnKKsrfeTZvJ"
      },
      "id": "MnKKsrfeTZvJ"
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What are the two main challenges that hinder the widespread application of the 'LLM-as-a-Judge' approach?\"\n",
        "\n",
        "prompt, sources = retrieve_and_construct_prompt(\n",
        "    question=user_question,\n",
        "    model=model,\n",
        "    index=index,\n",
        "    metadata=metadata,\n",
        "    top_k=5\n",
        ")\n",
        "answer = generate_answer_with_cleaning(generator, prompt)\n",
        "print(\"GENERATED ANSWER:\\n\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAMj9o8-L5Ni",
        "outputId": "e32eff33-28b4-4f13-a76d-df756d9d5494"
      },
      "id": "xAMj9o8-L5Ni",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED ANSWER:\n",
            "\n",
            "1. Reliability: LLMs' ability to mimic human-like reasoning and thinking processes is a significant\n",
            "challenge. LLMs' performance can vary significantly across different tasks, making it challenging to\n",
            "establish a reliable evaluation method.\n",
            "2. Robustness: LLMs' performance can also vary significantly\n",
            "across different tasks, making it challenging to establish a robust evaluation method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"How does LLM-as-a-Judge differ from traditional evaluation methods?\"\n",
        "\n",
        "prompt, sources = retrieve_and_construct_prompt(\n",
        "    question=user_question,\n",
        "    model=model,\n",
        "    index=index,\n",
        "    metadata=metadata,\n",
        "    top_k=5\n",
        ")\n",
        "answer = generate_answer_with_cleaning(generator, prompt)\n",
        "print(\"GENERATED ANSWER:\\n\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SpvEqGIT17O",
        "outputId": "2ee0d375-1a17-4e48-c306-3b93b9f82630"
      },
      "id": "6SpvEqGIT17O",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED ANSWER:\n",
            "\n",
            "LLM-as-a-Judge differs from traditional evaluation methods in several ways. Firstly, LLM-as-a-Judge\n",
            "is a\n",
            "system that evaluates the performance of a language model, rather than a human. This means that\n",
            "LLM-as-a-Judge\n",
            "is not evaluated by humans, but rather by a machine. Secondly, LLM-as-a-Judge is\n",
            "designed to evaluate the\n",
            "performance of a language model in a specific task, rather than evaluating\n",
            "the performance of a human\n",
            "judge. This means that LLM-as-a-Judge is not evaluated by humans, but\n",
            "rather by a machine. Thirdly, LLM-as-a-Judge\n",
            "is designed to be more robust and reliable than\n",
            "traditional evaluation methods. This is because LLM-as-a-Judge\n",
            "is designed to evaluate the\n",
            "performance of a language model in a specific task, rather than evaluating the\n",
            "performance of a\n",
            "human judge. This means that LLM-as-a-Judge is more likely to identify and mitigate biases and\n",
            "vulnerabilities in the language model. Finally, LLM-as-a-Judge is designed to be scalable and\n",
            "adaptable to different\n",
            "scenarios. This means that LLM-as-a-Judge can be used in a variety of\n",
            "different contexts and can be adapted to\n",
            "meet the specific needs of each context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What are the proposed future directions for improving LLM-as-a-Judge?\"\n",
        "\n",
        "prompt, sources = retrieve_and_construct_prompt(\n",
        "    question=user_question,\n",
        "    model=model,\n",
        "    index=index,\n",
        "    metadata=metadata,\n",
        "    top_k=5\n",
        ")\n",
        "answer = generate_answer_with_cleaning(generator, prompt)\n",
        "print(\"GENERATED ANSWER:\\n\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AikroW0UmE3",
        "outputId": "f917a540-4878-4d43-bc4f-bc70c8537ba5"
      },
      "id": "2AikroW0UmE3",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED ANSWER:\n",
            "\n",
            "The proposed future directions for improving LLM-as-a-Judge include:\n",
            "\n",
            "1. Improving the alignment\n",
            "with humans: The current evaluation methods are not fully aligned with humans, and further\n",
            "exploration is needed to improve the evaluation performance and mitigate biases.\n",
            "\n",
            "2. Developing\n",
            "evaluation methodologies: The development of evaluation methodologies has evolved significantly with\n",
            "the advent of GPT-4, enabling more scalable and flexible approaches to LLM-as-a-Judge systems. These\n",
            "evaluation paradigms typically rely on interactions with the environment to obtain feedback, which\n",
            "forms the foundation for self-evolution signals.\n",
            "\n",
            "3. Enhancing AI’s intelligent performance: If we\n",
            "can establish reliable LLM-as-a-Judge in the future, and further enhance AI’s intelligent\n",
            "performance as a World Model, using. World Model-as-a-Judge could make our simulations of the real\n",
            "world more realistic and widely reliable.\n",
            "\n",
            "4. Expanding applications: LLMs’ abilities as evaluators\n",
            "have gained widespread recognition in specialized fields, especially in complex, qualitative areas\n",
            "like legal texts, mathematical reasoning, and scientific research. This section reviews recent\n",
            "developments in LLM-as-a-judge applications across finance, law, science, and other industries,\n",
            "investigating how domain knowledge and LLM evaluators can further expand their impact in critical\n",
            "areas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future Improvements\n",
        "\n",
        "To enhance the RAG pipeline, several extensions could be considered:\n",
        "\n",
        "- **Query Reformulation**: Automatically rewrite user queries to better match document language, improving retrieval accuracy.\n",
        "- **Adaptive Retrieval**: Dynamically adjust the number of retrieved chunks based on query type or confidence scores.\n",
        "- **Grounded Evaluation**: Integrate factuality checks to ensure that generated answers are strictly based on retrieved evidence.\n",
        "\n",
        "These improvements would make the pipeline more robust, accurate, and closer to production-level applications.\n"
      ],
      "metadata": {
        "id": "xLXfaI5FaF6S"
      },
      "id": "xLXfaI5FaF6S"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}